{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from generate_bf import *\n",
    "from computesk import *\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from models.UMNN import MonotonicNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 10000\n",
    "num_pairs = 20\n",
    "num_features = num_pairs * 2\n",
    "hidden_layers = [64, 64, 64]\n",
    "nb_steps = 50\n",
    "lr = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_samples_path = 'BF/BF-samples/bf-testing_samples20p.txt'\n",
    "validation_samples_path = 'BF/BF-samples/bf-validation_samples20p.txt'\n",
    "save_path = 'BF/BF-saved_models/BF-20p-64_64_64.es' # Change this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_samples = generate_easy_bf(num_test, num_pairs)\n",
    "# validation_samples = generate_easy_bf(num_test, num_pairs)\n",
    "\n",
    "# np.savetxt(testing_samples_path, testing_samples)\n",
    "# np.savetxt(validation_samples_path, validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor(np.loadtxt(testing_samples_path)).to(torch.float32)\n",
    "validation_samples = torch.tensor(np.loadtxt(validation_samples_path)).to(torch.float32)\n",
    "\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [5000]\n",
    "random_regs = [1, 0.1, 0.01, 0.001, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kths = list(range(num_features))\n",
    "fixed_map = generate_non_linear_maps(num_features, hidden_layers, nb_steps, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_regs = {}\n",
    "all_test_losses = {}\n",
    "all_learnt_maps = {}\n",
    "all_opt_maps = {}\n",
    "test_no_reg_losses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Sk map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = training_sizes[0]\n",
    "X_tr = generate_easy_bf(num_train, num_pairs)\n",
    "\n",
    "for i in kths:\n",
    "    kth = i\n",
    "    print('kth =', kth)\n",
    "    best_val_overall = float('inf')\n",
    "    opt_reg = 0\n",
    "    opt_Sk = None # should this be moved in?\n",
    "    each_learnt_map = {}\n",
    "    non_kth = [idx for idx in range(X_tr.shape[1]) if idx != kth]\n",
    "\n",
    "    for j in tqdm(range(len(random_regs)), desc='Random Regs', leave=False):\n",
    "        regLambda = random_regs[j]\n",
    "        Sk = copy.deepcopy(fixed_map)[kth]\n",
    "        optimizer = optim.Adam(Sk.parameters(), lr=lr)\n",
    "        n = X_tr.shape[0]\n",
    "        early_stop_counter = 0\n",
    "        best_epoch = 0\n",
    "        best_valL = float('inf')\n",
    "        for epoch in range(num_epochs):\n",
    "            zk = X_tr.detach().requires_grad_(True)\n",
    "            h = zk[:, non_kth]\n",
    "            x = zk[:, [kth]]\n",
    "\n",
    "            sk_zi = Sk(x, h)\n",
    "            jacobian = torch.autograd.grad(sk_zi, x, torch.ones_like(sk_zi), create_graph=True)[0]\n",
    "            loss = (0.5 * sk_zi**2 - torch.log(jacobian)).sum(axis=0) / n #mapS_losses(sk_zi, jacobian).sum(axis=0) / n#, kth)\n",
    "            regulariser = torch.sqrt((jacobian**2).sum(axis=0) / n)\n",
    "            loss += regLambda * regulariser\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            Sk_zi_val, jacobian_val = test_map(validation_samples, non_kth, kth, Sk)\n",
    "            val_loss = test_losses(Sk_zi_val, jacobian_val)#, kth)\n",
    "            print(f'Val {num_train}st λ = {regLambda}, Epoch {epoch}: {val_loss}')\n",
    "\n",
    "            # Save the smallest validation loss at each loop.\n",
    "            if val_loss[1] < best_valL:\n",
    "                best_valL = val_loss[1]\n",
    "                if val_loss[1] < best_val_overall: # overall for all λ and epoch\n",
    "                    best_val_overall = val_loss[1]\n",
    "                    opt_reg = regLambda\n",
    "                    opt_Sk = Sk\n",
    "                    \n",
    "                best_epoch = epoch\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            # Check for early stopping\n",
    "            if early_stop_counter >= 10:\n",
    "                print(f'Early stopping at Epoch {epoch} for best epoch {best_epoch}.')\n",
    "                break\n",
    "\n",
    "        each_learnt_map.setdefault(regLambda, []).append(Sk)\n",
    "\n",
    "    # Test the best model\n",
    "    Sk_zi_test, jacobian_test = test_map(test_samples, non_kth, kth, opt_Sk)\n",
    "\n",
    "    all_test_losses.setdefault(num_train, []).append(test_losses(Sk_zi_test, jacobian_test))###, kth))\n",
    "    print(f'Test {num_train}, λ = {opt_reg}: {all_test_losses[num_train]}')\n",
    "\n",
    "    all_learnt_maps.setdefault(num_train, []).append(each_learnt_map)\n",
    "    all_opt_maps.setdefault(num_train, []).append(opt_Sk)\n",
    "    opt_regs.setdefault(num_train, []).append(opt_reg)\n",
    "    \n",
    "print('Optimal λ ∀ =', opt_regs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_test_losses[num_train]\n",
    "\n",
    "# Sum of the second value of each tuple\n",
    "total = sum(t[1] for t in data)\n",
    "print(\"Total NLL (computed):\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = save_path + '/optimal_reg'\n",
    "os.makedirs(save_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in all_opt_maps.items():\n",
    "    save_path_here = os.path.join(save_directory, 'tr' + f'{model_name}.pth')\n",
    "    state_dicts = {}\n",
    "    for i in range(len(model)):\n",
    "        state_dicts[f'model{i+1}_state_dict'] = model[i].state_dict()\n",
    "    torch.save(state_dicts, save_path_here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory_no_reg = save_path + '/all_reg'\n",
    "os.makedirs(save_directory_no_reg, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_organised_learners = {}\n",
    "\n",
    "for num_train, learnt_maps_list in all_learnt_maps.items():\n",
    "    print(num_train)\n",
    "    all_learnt_maps_org = {}\n",
    "    for learnt_map_dict in learnt_maps_list:\n",
    "        for reg_lambda, model_list in learnt_map_dict.items():\n",
    "            print(reg_lambda)\n",
    "            print(model_list)\n",
    "            all_learnt_maps_org.setdefault(reg_lambda, []).append(model_list[0])\n",
    "    all_organised_learners[num_train] = all_learnt_maps_org\n",
    "\n",
    "for training_size, regularization_dict in all_organised_learners.items():\n",
    "    training_size_folder = os.path.join(save_directory_no_reg, 'tr' + str(training_size))\n",
    "    os.makedirs(training_size_folder, exist_ok=True)\n",
    "\n",
    "    for regularization, model in regularization_dict.items():\n",
    "        model_name = f'reg{regularization}.pth'\n",
    "        model_path = os.path.join(training_size_folder, model_name)\n",
    "        state_dicts = {}\n",
    "        for i in range(len(model)):\n",
    "            state_dicts[f'model{i+1}_state_dict'] = model[i].state_dict()\n",
    "        torch.save(state_dicts, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = torch.load(save_directory + '/tr5000.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_state_dicts = [loaded_models[f'model{i}_state_dict'] for i in range(1, num_features + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "\n",
    "for i in range(num_features):\n",
    "    model = MonotonicNN(num_features, hidden_layers, nb_steps, 'cpu')\n",
    "    model.load_state_dict(model_state_dicts[i])\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "precision_matrix = []\n",
    "\n",
    "\n",
    "for j in range(num_features):\n",
    "    Sj = models[j]\n",
    "    row = []\n",
    "    Sj.eval()\n",
    "    kth = j\n",
    "    non_kth = [idx for idx in range(test_samples.shape[1]) if idx != kth]\n",
    "\n",
    "    zk = test_samples.detach().requires_grad_(True)\n",
    "    h = zk[:, non_kth]\n",
    "    x = zk[:, [kth]]\n",
    "    \n",
    "    sk_zi = Sj(x,h)  \n",
    "    for i in range(num_features):\n",
    "        print(i, j)\n",
    "        if i != j:\n",
    "            first_derivative = torch.autograd.grad(sk_zi, zk, torch.ones_like(sk_zi), create_graph=True)[0]\n",
    "            first_derivative = torch.log(torch.abs(first_derivative))\n",
    "            second_derivative = torch.autograd.grad(first_derivative[:, [j]], zk, torch.ones_like(first_derivative[:, [j]]), create_graph=True)[0] # check whether they are columns or row vector. column vector by adding [j]!!! This is currently a row vevotr which isn't right!\n",
    "            third_derivative= torch.autograd.grad(second_derivative[:, [j]], zk, torch.ones_like(second_derivative[:, [j]]), create_graph=True)[0]\n",
    "\n",
    "            second = torch.abs(third_derivative[:,[i]]).mean().item()\n",
    "            first_half = -1/2 * (sk_zi**2)\n",
    "            first_half_derivative = torch.autograd.grad(first_half, zk, torch.ones_like(first_half), create_graph=True)[0]\n",
    "            second_half_deriative = torch.autograd.grad(first_half_derivative[:, [j]], zk, torch.ones_like(first_half_derivative[:, [j]]), create_graph=True)[0]\n",
    "\n",
    "            first = torch.abs(second_half_deriative[:, [i]]).mean().item()\n",
    "            row.append(first + second)\n",
    "        else:\n",
    "            row.append(1) \n",
    "    precision_matrix.append(row)\n",
    "    print(precision_matrix[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array(precision_matrix)\n",
    "transpose_matrix = matrix.T \n",
    "symmetric_matrix = (transpose_matrix + matrix) / 2\n",
    "print(symmetric_matrix)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xticks(np.arange(0, len(symmetric_matrix), 1), np.arange(1, len(symmetric_matrix) + 1))\n",
    "plt.yticks(np.arange(0, len(symmetric_matrix), 1), np.arange(1, len(symmetric_matrix) + 1))\n",
    "plt.imshow(symmetric_matrix, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise with maximum value of the matrix.\n",
    "max_value = np.max(symmetric_matrix)\n",
    "\n",
    "normalized_matrix = symmetric_matrix / max_value\n",
    "\n",
    "np.fill_diagonal(normalized_matrix, 1)\n",
    "\n",
    "print(\"Normalized matrix with diagonals set to 1:\")\n",
    "print(normalized_matrix)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xticks(np.arange(0, len(normalized_matrix), 1), np.arange(1, len(normalized_matrix) + 1))\n",
    "plt.yticks(np.arange(0, len(normalized_matrix), 1), np.arange(1, len(normalized_matrix) + 1))\n",
    "plt.imshow(normalized_matrix, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_value = np.max(symmetric_matrix)\n",
    "\n",
    "normalized_matrix = symmetric_matrix / max_value\n",
    "\n",
    "np.fill_diagonal(normalized_matrix, 1)\n",
    "\n",
    "print(\"Normalized matrix with diagonals set to 1:\")\n",
    "print(normalized_matrix)\n",
    "\n",
    "# Create ticks for odd numbers only\n",
    "ticks = np.arange(1, len(normalized_matrix) + 1, 2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xticks(np.arange(0, len(normalized_matrix), 2), ticks)\n",
    "plt.yticks(np.arange(0, len(normalized_matrix), 2), ticks)\n",
    "plt.imshow(normalized_matrix, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "matrix = np.array(precision_matrix)\n",
    "transpose_matrix = matrix.T \n",
    "symmetric_matrix = (transpose_matrix + matrix) / 2\n",
    "\n",
    "# Find the largest non-diagonal value\n",
    "max_value = np.max(np.abs(np.triu(symmetric_matrix, k=1)))\n",
    "\n",
    "# Divide the matrix by the largest non-diagonal value\n",
    "if max_value != 0:\n",
    "    symmetric_matrix /= max_value\n",
    "\n",
    "\n",
    "np.fill_diagonal(symmetric_matrix, 1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xticks(np.arange(0, len(symmetric_matrix), 1), np.arange(1, len(symmetric_matrix) + 1))\n",
    "plt.yticks(np.arange(0, len(symmetric_matrix), 1), np.arange(1, len(symmetric_matrix) + 1))\n",
    "plt.imshow(symmetric_matrix, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symmetric_matrix_rounded = np.round(symmetric_matrix, decimals=3)\n",
    "\n",
    "# Print the symmetric matrix with rounded values\n",
    "print(symmetric_matrix_rounded)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xticks(np.arange(0, len(symmetric_matrix), 1), np.arange(1, len(symmetric_matrix) + 1))\n",
    "plt.yticks(np.arange(0, len(symmetric_matrix), 1), np.arange(1, len(symmetric_matrix) + 1))\n",
    "plt.imshow(symmetric_matrix_rounded, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(symmetric_matrix)):\n",
    "    for j in range(len(symmetric_matrix[0])):\n",
    "        plt.text(j, i, f'{symmetric_matrix_rounded[i, j]:.3f}', ha='center', va='center', color='white')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold_values = [0.3, 0.2, 0.1, 0.05]\n",
    "\n",
    "matrix_size = 40\n",
    "\n",
    "ground_truth = np.eye(matrix_size)  # Start with the identity matrix (1s on the diagonal)\n",
    "\n",
    "# Set the specific pairs to 1\n",
    "for i in range(0, matrix_size, 2):\n",
    "    ground_truth[i, i+1] = 1\n",
    "    ground_truth[i+1, i] = 1  # Ensure symmetry\n",
    "\n",
    "# Calculate the total number of negative entries in the ground truth matrix\n",
    "total_negatives = np.sum(ground_truth == 0)\n",
    "\n",
    "# Initialize a plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Loop through each threshold value\n",
    "for threshold in threshold_values:\n",
    "    false_positive_rates_list = []\n",
    "\n",
    "    # Loop through training sizes and compute false positive rates\n",
    "    for training_size in training_sizes:\n",
    "        # Load the symmetric matrix\n",
    "        matrix = symmetric_matrix\n",
    "        # Normalize the matrix by its largest value\n",
    "        max_value = np.max(matrix)\n",
    "        normalized_matrix = matrix / max_value\n",
    "\n",
    "        # Apply the threshold to create a binary matrix\n",
    "        binary_matrix = np.where(normalized_matrix < threshold, 0, 1)\n",
    "\n",
    "        # Compute the false positives\n",
    "        false_positives = np.sum((ground_truth == 0) & (binary_matrix == 1))\n",
    "\n",
    "        # Calculate the false positive rate\n",
    "        false_positive_rate = false_positives / total_negatives\n",
    "\n",
    "        # Store the false positive rate\n",
    "        false_positive_rates_list.append(false_positive_rate)\n",
    "        print(false_positive_rates_list)\n",
    "    # Plot the results for the current threshold\n",
    "    plt.plot(training_sizes, false_positive_rates_list, marker='o', linestyle='-', label=f'Threshold = {threshold:.2f}')\n",
    "\n",
    "# Customize the plot appearance\n",
    "plt.xlabel('Training Size', fontsize=14)\n",
    "plt.ylabel('False Positive Rate', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title='Threshold Values', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f'{save_directory}/false_positive_rates_vs_training_size_thresholds.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold_values = [0.3, 0.2, 0.1, 0.05]\n",
    "\n",
    "matrix_size = 40\n",
    "\n",
    "ground_truth = np.eye(matrix_size)  # Start with the identity matrix (1s on the diagonal)\n",
    "\n",
    "# Set the specific pairs to 1\n",
    "for i in range(0, matrix_size, 2):\n",
    "    ground_truth[i, i+1] = 1\n",
    "    ground_truth[i+1, i] = 1  # Ensure symmetry\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    f1_scores_list = []\n",
    "    for training_size in training_sizes:\n",
    "        matrix = symmetric_matrix\n",
    "        max_value = np.max(matrix)\n",
    "        normalized_matrix = matrix / max_value\n",
    "\n",
    "        binary_matrix = np.where(normalized_matrix < threshold, 0, 1)\n",
    "\n",
    "        true_positives = np.sum((ground_truth == 1) & (binary_matrix == 1))\n",
    "        false_positives = np.sum((ground_truth == 0) & (binary_matrix == 1))\n",
    "        false_negatives = np.sum((ground_truth == 1) & (binary_matrix == 0))\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        f1_scores_list.append(f1_score)\n",
    "        print(f1_scores_list)\n",
    "\n",
    "    plt.plot(training_sizes, f1_scores_list, marker='o', linestyle='-', label=f'Threshold = {threshold:.2f}')\n",
    "\n",
    "plt.xlabel('Training Size', fontsize=14)\n",
    "plt.ylabel('F1 Score', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title='Threshold Values', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'{save_directory}/f1_scores_vs_training_size_thresholds.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
